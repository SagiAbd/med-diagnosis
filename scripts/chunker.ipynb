{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Med-Diagnosis: Semantic Chunking + Embedding Ingestion\n\n**Input:** `corpus_full_text.jsonl`  \nEach line: `{\"protocol_id\": \"p_...\", \"text\": \"...\", ...}`\n\n**Outputs:**\n- `corpus.json` → copy to `./backend/data/corpus.json`  \n- `chroma_export.zip` → extract as `./chroma_data/` in project root\n\nOn next app startup, `corpus_loader.py` reads `corpus.json` for SQL records  \nand skips Chroma embedding (vectors already present)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q chromadb>=0.6.3 langchain-chroma>=0.0.5 langchain-huggingface langchain-openai sentence-transformers tiktoken"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── CONFIG ────────────────────────────────────────────────────────────────\n\n# Input: raw full-text protocols (before cleaning)\nRAW_JSONL = \"/content/corpus_full_text_raw.jsonl\"\n\n# Cleaned output (preamble + postamble stripped) — also used for chunking\nCORPUS_FULL_TEXT_JSONL = \"/content/corpus_full_text.jsonl\"\n\n# Outputs\nCHROMA_PATH = \"/content/chroma_data\"   # → ./chroma_data/ in project\nCORPUS_JSON = \"/content/corpus.json\"   # → ./backend/data/corpus.json\n\n# Must match CHROMA_COLLECTION_NAME in .env (default: \"documents\")\nCOLLECTION_NAME = \"documents\"\n\n# ── Chunking ──\nCHUNK_SIZE = 300   # tokens per chunk\nCHUNK_OVERLAP = 50  # token overlap between chunks\n\n# ── Embeddings ──\n# \"huggingface\" : local GPU inference (must use the SAME model as TEI)\n# \"openai\"      : OpenAI-compatible API\nEMBEDDINGS_MODE  = \"huggingface\"\nEMBEDDINGS_MODEL = \"google/embeddinggemma-300m\"  # must match TEI MODEL_ID\nHF_TOKEN         = \"your-hf-token-here\"\n\n# OpenAI (only if EMBEDDINGS_MODE=\"openai\")\nOPENAI_API_KEY   = \"\"\nOPENAI_API_BASE  = \"https://api.openai.com/v1\"\nOPENAI_EMB_MODEL = \"text-embedding-ada-002\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Imports ───────────────────────────────────────────────────────────────\nimport hashlib, json, os, re, shutil\nimport chromadb\nimport tiktoken\nfrom typing import List, Dict\nfrom langchain_chroma import Chroma\nfrom langchain_core.documents import Document as LangchainDocument\n\nos.makedirs(CHROMA_PATH, exist_ok=True)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── SemanticChunker ───────────────────────────────────────────────────────\n#\n# Two distinct protocol formats observed in the corpus:\n#\n# FORMAT A (2015 style):\n#   I. ВВОДНАЯ ЧАСТЬ          ← Roman-numeral major section\n#   II. МЕТОДЫ ДИАГНОСТИКИ\n#   III. ОРГАНИЗАЦИОННЫЕ АСПЕКТЫ  ← administrative tail → strip\n#   Items inside sections are flat-numbered: 1. 2. 3. ... (NOT section boundaries)\n#\n# FORMAT B (2017 style):\n#   1. ВВОДНАЯ ЧАСТЬ          ← top-level decimal section (e.g. \"1.\")\n#   1.1 Код(ы) МКБ-10         ← subsection (NOT a boundary)\n#   1.9 МЕТОДЫ, ПОДХОДЫ...    ← subsection, may be ALL-CAPS\n#   2.1 Диагностический алгоритм\n#   3.1 ТАКТИКА ЛЕЧЕНИЯ...    ← subsection\n#   6. ОРГАНИЗАЦИОННЫЕ АСПЕКТЫ  ← administrative tail → strip\n#\n# Strategy:\n#   1. Strip boilerplate preamble (\"Одобрен/Рекомендовано ... Протокол №NN\")\n#   2. Detect format (Roman vs decimal) from text\n#   3. For Format A: split on Roman-numeral section headers + ALL-CAPS title lines\n#   4. For Format B: split on top-level decimal sections ONLY (not subsections like \"1.1\")\n#   5. Strip the administrative tail before chunking (authors, references, reviewers)\n#   6. Large sections → sliding-window with overlap; small → keep atomic\n\nimport re\nfrom typing import List, Dict\nimport tiktoken\n\n# ── Regex patterns ─────────────────────────────────────────────────────────\n\n_ROMAN = r\"(?:X{0,3}(?:IX|IV|V?I{0,3}))\"\n\n# Format A: Roman numerals (I. II. III. ...) and ALL-CAPS title lines\n_FORMAT_A_RE = re.compile(\n    r\"(?m)^(?:\"\n    r\"#{1,3}\\s+\"\n    rf\"|{_ROMAN}\\.\\s+\"                          # I. II. III. …\n    r\"|[А-ЯЁA-Z][А-ЯЁA-Z\\s\\-\\/]{5,}$\"         # ALL-CAPS Cyrillic/Latin ≥6 chars\n    r\")\"\n)\n\n# Format B: top-level decimal ONLY — \"1. \" or \"2. \" at line start,\n# but NOT \"1.1\" / \"2.3\" (subsections)\n_FORMAT_B_RE = re.compile(\n    r\"(?m)^\\d+\\.\\s+(?!\\d)\"                     # \"3. \" yes, \"3.1 \" no\n)\n\n# Preamble: boilerplate header before the protocol title\n_PREAMBLE_RE = re.compile(\n    r\"^.*?(?=КЛИНИЧЕСКИЙ ПРОТОКОЛ|КЛИНИЧЕСКИЕ РЕКОМЕНДАЦИИ|ПРОТОКОЛ ДИАГНОСТИКИ)\",\n    re.DOTALL | re.IGNORECASE,\n)\n\n# Administrative tail markers — strip everything from here to end\n# Both formats: Roman III + decimal 6 are typically the admin section\n_TAIL_RE = re.compile(\n    r\"(?m)(?:\"\n    r\"^III\\.\\s+ОРГАНИЗАЦИОННЫЕ\"            # Format A tail\n    r\"|^VI?\\.\\s+ОРГАНИЗАЦИОННЫЕ\"           # Format B tail (section 6 or 7)\n    r\"|^(?:6|7)\\.\\s+ОРГАНИЗАЦИОННЫЕ\"       # decimal \"6. ОРГАНИЗАЦИОННЫЕ...\"\n    r\")\",\n    re.IGNORECASE,\n)\n\n\nclass SemanticChunker:\n    def __init__(self, chunk_size: int = 300, overlap: int = 50):\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.encoder = tiktoken.get_encoding(\"cl100k_base\")\n\n    def chunk_document(self, text: str, metadata: Dict) -> List[Dict]:\n        text = self._strip_preamble(text)\n        text = self._strip_admin_tail(text)\n        fmt = self._detect_format(text)\n        sections = self._detect_sections(text, fmt)\n        chunks = []\n        for section in sections:\n            if self._is_atomic_section(section):\n                chunks.append(self._create_chunk(section, metadata))\n            else:\n                chunks.extend(\n                    self._create_chunk(sub, metadata)\n                    for sub in self._split_with_overlap(section, self.chunk_size, self.overlap)\n                )\n        return chunks\n\n    # ── Pre-processing ──────────────────────────────────────────────────────\n\n    def _strip_preamble(self, text: str) -> str:\n        m = _PREAMBLE_RE.match(text)\n        return text[m.end():].strip() if m and m.end() > 0 else text\n\n    def _strip_admin_tail(self, text: str) -> str:\n        m = _TAIL_RE.search(text)\n        return text[: m.start()].strip() if m else text\n\n    # ── Format detection ────────────────────────────────────────────────────\n\n    def _detect_format(self, text: str) -> str:\n        \"\"\"Return 'A' if Roman-numeral sections found, else 'B' (decimal).\"\"\"\n        roman_hits = re.findall(rf\"(?m)^{_ROMAN}\\.\\s+\", text)\n        return \"A\" if len(roman_hits) >= 2 else \"B\"\n\n    # ── Section detection ───────────────────────────────────────────────────\n\n    def _detect_sections(self, text: str, fmt: str) -> List[str]:\n        pattern = _FORMAT_A_RE if fmt == \"A\" else _FORMAT_B_RE\n        matches = list(pattern.finditer(text))\n        if not matches:\n            return [text.strip()] if text.strip() else []\n\n        sections = []\n        preamble = text[: matches[0].start()].strip()\n        if preamble:\n            sections.append(preamble)\n\n        for i, m in enumerate(matches):\n            end = matches[i + 1].start() if i + 1 < len(matches) else len(text)\n            section = text[m.start() : end].strip()\n            if section:\n                sections.append(section)\n\n        return sections\n\n    # ── Chunking helpers ────────────────────────────────────────────────────\n\n    def _is_atomic_section(self, section: str) -> bool:\n        return len(self.encoder.encode(section)) <= self.chunk_size\n\n    def _split_with_overlap(self, text: str, size: int, overlap: int) -> List[str]:\n        tokens = self.encoder.encode(text)\n        chunks = []\n        step = max(size - overlap, 1)\n        for i in range(0, len(tokens), step):\n            chunk_tokens = tokens[i : i + size]\n            chunks.append(self.encoder.decode(chunk_tokens))\n        return chunks\n\n    def _create_chunk(self, text: str, metadata: Dict) -> Dict:\n        return {\n            \"text\": text,\n            \"metadata\": {\n                **metadata,\n                \"chunk_size\": len(self.encoder.encode(text)),\n                \"preview\": text[:100] + \"...\",\n            },\n        }\n\n\nchunker = SemanticChunker(chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP)\nprint(\"SemanticChunker ready\")\n\n# ── Sanity check: Format A (2015) ─────────────────────────────────────────\n_SAMPLE_A = \"\"\"Рекомендовано Экспертным советом РГП на ПХВ «Республиканский центр» от «20» ноября 2015 года Протокол № 16 КЛИНИЧЕСКИЙ ПРОТОКОЛ ДИАГНОСТИКИ И ЛЕЧЕНИЯ ЭНТЕРОВИРУСНАЯ ИНФЕКЦИЯ У ДЕТЕЙ I.ВВОДНАЯ ЧАСТЬ 1.Название протокола: Энтеровирусная инфекция у детей. 2.Код протокола: 3.Код (коды) по МКБ – 10: А85.0 Энтеровирусный энцефалит II. МЕТОДЫ, ПОДХОДЫ И ПРОЦЕДУРЫ ДИАГНОСТИКИ И ЛЕЧЕНИЯ 8.Определение: Энтеровирусная инфекция – заболевание... III. ОРГАНИЗАЦИОННЫЕ АСПЕКТЫ ВНЕДРЕНИЯ ПРОТОКОЛА: 16.Список разработчиков протокола...\"\"\"\n_SAMPLE_B = \"\"\"Одобрен Объединенной комиссией по качеству медицинских услуг от «29» июня 2017 года Протокол № 24 КЛИНИЧЕСКИЙ ПРОТОКОЛ ДИАГНОСТИКИ И ЛЕЧЕНИЯ ЭНТЕРОБИОЗУ ДЕТЕЙ 1. ВВОДНАЯ ЧАСТЬ 1.1 Код(ы) МКБ-10: МКБ-10 Код Название В80 Энтеробиоз 1.7 Определение: Энтеробиоз – гельминтоз... 2. МЕТОДЫ ДИАГНОСТИКИ 2.1 Диагностический алгоритм: схема... 3. ТАКТИКА ЛЕЧЕНИЯ 3.1 ТАКТИКА ЛЕЧЕНИЯ НА АМБУЛАТОРНОМ УРОВНЕ: Лечение энтеробиоза... 6. ОРГАНИЗАЦИОННЫЕ АСПЕКТЫ ПРОТОКОЛА: 6.1 Список разработчиков...\"\"\"\n\nfor label, sample in [(\"A (Roman)\", _SAMPLE_A), (\"B (Decimal)\", _SAMPLE_B)]:\n    tc = SemanticChunker(chunk_size=300, overlap=50)\n    stripped = tc._strip_preamble(sample)\n    stripped = tc._strip_admin_tail(stripped)\n    fmt = tc._detect_format(stripped)\n    sections = tc._detect_sections(stripped, fmt)\n    print(f\"\\nFormat {label} → detected: '{fmt}'\")\n    print(f\"After stripping tail, sections: {len(sections)}\")\n    for i, s in enumerate(sections):\n        print(f\"  [{i}] {s[:80]}...\")"
  },
  {
   "cell_type": "code",
   "source": "# ── Clean corpus_full_text.jsonl ──────────────────────────────────────────\n# Reads RAW_JSONL, strips preamble + administrative postamble from each\n# protocol's text, writes cleaned records to CORPUS_FULL_TEXT_JSONL.\n#\n# Uses _PREAMBLE_RE and _TAIL_RE defined in the SemanticChunker cell above.\n\ndef _clean_protocol_text(text: str) -> str:\n    m = _PREAMBLE_RE.match(text)\n    if m and m.end() > 0:\n        text = text[m.end():].strip()\n    m = _TAIL_RE.search(text)\n    if m:\n        text = text[: m.start()].strip()\n    return text\n\n\nskipped = 0\ncleaned_protocols: list[dict] = []\ntotal_removed = 0\n\nwith open(RAW_JSONL, encoding=\"utf-8\", errors=\"replace\") as f:\n    for line in f:\n        line = line.strip()\n        if not line:\n            continue\n        try:\n            obj = json.loads(line)\n        except json.JSONDecodeError:\n            skipped += 1\n            continue\n\n        original_text = obj.get(\"text\", \"\")\n        cleaned_text = _clean_protocol_text(original_text)\n        total_removed += len(original_text) - len(cleaned_text)\n        obj[\"text\"] = cleaned_text\n        cleaned_protocols.append(obj)\n\nwith open(CORPUS_FULL_TEXT_JSONL, \"w\", encoding=\"utf-8\") as f:\n    for obj in cleaned_protocols:\n        f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Cleaned {len(cleaned_protocols)} protocols → {CORPUS_FULL_TEXT_JSONL}\")\nprint(f\"Total chars removed: {total_removed:,} ({total_removed / max(len(cleaned_protocols),1):.0f} avg per protocol)\")\nif skipped:\n    print(f\"Skipped {skipped} malformed lines\")\n\n# Sanity: show before/after for first protocol\nif cleaned_protocols:\n    s = cleaned_protocols[0]\n    print(f\"\\nprotocol_id: {s.get('protocol_id')}\")\n    print(f\"Cleaned preview:\\n{s['text'][:400]}...\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Use cleaned protocols for chunking ────────────────────────────────────\nprotocols = cleaned_protocols\nprint(f\"Loaded {len(protocols)} protocols\")\nprint(\"Sample protocol_id:\", protocols[0].get(\"protocol_id\"), \"| text length:\", len(protocols[0].get(\"text\", \"\")))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Chunk all protocols ───────────────────────────────────────────────────\nraw_chunks: list[dict] = []\n\nfor protocol in protocols:\n    protocol_id = protocol.get(\"protocol_id\", \"\")\n    text = protocol.get(\"text\", \"\")\n    if not text.strip():\n        continue\n\n    # Base metadata carried into every chunk\n    metadata = {\n        \"protocol_id\": protocol_id,\n        \"source\": \"corpus\",\n    }\n\n    chunks = chunker.chunk_document(text, metadata)\n    raw_chunks.extend(chunks)\n\nprint(f\"Total chunks produced: {len(raw_chunks)}\")\nprint(f\"Avg chunks per protocol: {len(raw_chunks) / max(len(protocols), 1):.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Convert to LangChain format + assign chunk IDs ────────────────────────\n# chunk_id formula must match corpus_loader.py: SHA256(f\"{source}:{page_content}\")\nseen_ids: set[str] = set()\nlc_chunks: list[dict] = []  # LangChain format: page_content + metadata + _id\n\nfor c in raw_chunks:\n    page_content = c[\"text\"]\n    source = c[\"metadata\"].get(\"source\", \"corpus\")\n    chunk_id = hashlib.sha256(f\"{source}:{page_content}\".encode()).hexdigest()\n\n    if chunk_id in seen_ids:\n        continue\n    seen_ids.add(chunk_id)\n\n    lc_chunks.append({\n        \"page_content\": page_content,\n        \"metadata\": c[\"metadata\"],\n        \"_id\": chunk_id,\n    })\n\nprint(f\"Unique chunks after dedup: {len(lc_chunks)}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Embeddings model ──────────────────────────────────────────────────────\nif EMBEDDINGS_MODE == \"huggingface\":\n    import torch\n    from langchain_huggingface import HuggingFaceEmbeddings\n    os.environ[\"HF_TOKEN\"] = HF_TOKEN\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    print(f\"Device: {device}\")\n    embeddings = HuggingFaceEmbeddings(\n        model_name=EMBEDDINGS_MODEL,\n        model_kwargs={\"device\": device, \"token\": HF_TOKEN},\n        encode_kwargs={\"normalize_embeddings\": True},\n    )\nelif EMBEDDINGS_MODE == \"openai\":\n    from langchain_openai import OpenAIEmbeddings\n    embeddings = OpenAIEmbeddings(\n        api_key=OPENAI_API_KEY, base_url=OPENAI_API_BASE, model=OPENAI_EMB_MODEL,\n    )\nelse:\n    raise ValueError(f\"Unknown EMBEDDINGS_MODE: {EMBEDDINGS_MODE}\")\n\nprint(f\"Embedding dim: {len(embeddings.embed_query('test'))}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Chroma setup ──────────────────────────────────────────────────────────\nchroma_client = chromadb.PersistentClient(path=CHROMA_PATH)\nvector_store  = Chroma(\n    client=chroma_client,\n    collection_name=COLLECTION_NAME,\n    embedding_function=embeddings,\n)\nexisting_ids = set(vector_store._collection.get()[\"ids\"])\nprint(f\"Collection '{COLLECTION_NAME}': {len(existing_ids)} existing vectors\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ── Embed & store (skips already-present chunks) ──────────────────────────\nto_embed = [\n    LangchainDocument(\n        page_content=c[\"page_content\"],\n        metadata=c[\"metadata\"],\n        id=c[\"_id\"],\n    )\n    for c in lc_chunks if c[\"_id\"] not in existing_ids\n]\nprint(f\"{len(to_embed)} new chunks to embed  ({len(lc_chunks) - len(to_embed)} already in Chroma)\")\n\nBATCH = 100\nfor i in range(0, len(to_embed), BATCH):\n    batch = to_embed[i : i + BATCH]\n    vector_store.add_documents(batch, ids=[d.id for d in batch])\n    print(f\"  {min(i + BATCH, len(to_embed))}/{len(to_embed)}\")\n\nprint(f\"Done. Collection total: {vector_store._collection.count()} vectors\")"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# ── Save corpus.json (strip internal _id key) ─────────────────────────────\ncorpus = [{\"page_content\": c[\"page_content\"], \"metadata\": c[\"metadata\"]} for c in lc_chunks]\nwith open(CORPUS_JSON, \"w\", encoding=\"utf-8\") as f:\n    json.dump(corpus, f, ensure_ascii=False, indent=2)\nprint(f\"Saved {len(corpus)} chunks → {CORPUS_JSON}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## Deploy\n```bash\ncp corpus.json ./backend/data/corpus.json\nunzip -o chroma_export.zip -d .\ndocker compose -f docker-compose.dev.cpu.yml up -d --force-recreate backend\n```",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# ── Package & download ────────────────────────────────────────────────────\nshutil.make_archive(\"/content/chroma_export\", \"zip\", \"/content\", \"chroma_data\")\nprint(\"Zipped → /content/chroma_export.zip\")\n\ntry:\n    from google.colab import files\n    files.download(CORPUS_JSON)\n    files.download(\"/content/chroma_export.zip\")\nexcept ImportError:\n    print(f\"Files ready:\\n  {CORPUS_JSON}\\n  /content/chroma_export.zip\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}